\documentclass[../main.tex]{subfiles}

\begin{document}

\begin{itemize}
  \item We are in a world of big data
  \item ML helps to detect pattern in data, and we use that pattern to predict the future and perform decision making under uncertainty
  \item Supervised learning tries to learn mapping between inputs $x$ to outputs $y$
  \item Classification = categorical output
  \item Regression = scalar output
  \item Supervised learning can also be formalized as function approximation: $y = f(x)$ and we want to estimate $f$ using a labeled training set, $p(y_{i}|x_{i}, \theta)$
  \item MAP estimate
  \item Google SmartASS predicts probability you will click on an ad based on your search history
  \item Unsupervised learning's goal is to find interesting pattern in the data, also known as knowledge discovery
  \item Can be formalized as density estimation, $p(x_{i}|\theta)$
  \item Canonical example of unsupervised learning is clustering data
  \item Model-based clustering - fit a probabilistic model to the data
  \item Dimensionality reduction - project high dimensional data to a lower dimension to capture "essence" of the data
  \item Data may be high dim, but there may only be a small \# of degrees of variability, latent factors
  \item E.g. face images - latent factors are lighting, pose, identity, etc
  \item Discovering graph structure, discover new knowledge and get better joint probability density estimators
  \item Matrix completion or imputation, fill in missing entries with plausible values
  \item E.g. in collaborative filtering we have a matrix $X$ where $X(m,u)$ is the rating by user $u$ of movie $m$
  \item We only observe a tiny subset of $X$ matrix, and we want to predict the rest
  \item Reinforcement learning is learning how to act when given occasional reward signals
  \item If there is a fixed \# of parameters then it is a parametric model. If the \$ of parameters grow with training data, this is a non-parametric model
  \item KNN is a non-parametric classifier and an example of instance-based learning
  \item Common distance metric to use is the Euclidean distance
  \item If K = 1, then we have a Voronoi tesselation of the points
  \item Curse of dimensionality
  \item Main way to combat (CoD) is to make assumptions about the data and these assumptions are \textbf{inductive bias} and often in the form of parametric models
  \item E.g. linear regression: $y(x) = w^{T}x + \epsilon$ and the residual error term is normally distributed
  \item Logistic regresson: binary classification, use a Bernoulli distribution instead
  \item Data is not linearly separable
\end{itemize}



\end{document}