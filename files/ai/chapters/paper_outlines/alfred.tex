\begin{itemize}
  \item Action Learning From Realistic Environments and Directives (ALFRED)
  \item Mapping natural language instructions and vision to actions for household tasks
  \item Contains long, compositional tasks to bridge gap between research and real-world applications
  \item Language directives have high-level goals and low-level language instructions
  \item Baseline model performs poorly on the ALFRED dataset, more room for grounded visual language models
  \item Lot of platforms for language-driven navigation, embodied QA
  \item 8k demonstrations, average of 50 steps
  \item Interact with objects by specifying a pixelwise interaction mask, unlike other settings where they treat localization as a solved problem
  \item Evaluate on seq2seq model like VLN tasks which is not effective achieving less than 5\% success rate
  \item Need models that can address challenges of language, action, and vision for accomplishing household tasks
  \item Objects contain multiple visual variations different shapes, textures, and colors
  \item Tasks are parameterized by object of focus
  \item Generate expert demonstrations by encoding agent and environment dynamics into PDDL rules
  \item Split validation and test into \textit{seen} and \textit{unseen} environments to test models generalization to new spaces and novel object classes
  \item Baseline model: seq2seq architecture
  \begin{itemize}
    \item CNN encodes visual input
    \item bi-LSTM encodes the language directive
    \item decoder LSTM infers low-level actions and attends over encoded language
  \end{itemize}
  \item Model is trained using imitation learning, DAgger-style is non-trivial
  \item High-level language is concatenated with low-level language with a seperator
  \item Tasks require reasoning over long sequences of images and instructions, proposed two auxillary losses to add temporal information (progress monitoring)
  \item Progress monitoring is where the agent maintains an internal estimate of their progress towards the goal and subgoals
  \item Evaluate both Task Success and Goal-Condition success
  \item Seq2seq model only achieved 8 percent goal-conditioned success rate
  \item Also conducted some ablations to investigate unimodal ablation, found that both language and vision was necessary and a single modal is not sufficient to complete the task
  \item Possible directions: models that exploit hierarchy, are modular, and support structured reasoning and planning
\end{itemize}