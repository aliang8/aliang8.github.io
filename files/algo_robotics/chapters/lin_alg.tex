\documentclass[../main.tex]{subfiles}

\begin{document}
  Matrices are the fundamental representation for data in robotic applications.

  \section{Vectors and vector spaces}

  \textbf{Scalar}: a single number (e.g. 1.234) \\
  \textbf{Vector}: an ordered list of \textit{n} scalars where \textit{n} is the dimensionality (e.g. $\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$) \\
  Vectors can be interpreted as arrows in an n-dimensional vector space. [Insert diagram]

  \textbf{Vector operations} \\
  Vectors must have the same dimensionality.
  \begin{itemize}
      \item Addition: $v + w = \begin{bmatrix} v_{1} + w_{1} \\ v_{2} + w_{2} \\ \vdots \end{bmatrix}$
      \item Subtraction: $v - w = \begin{bmatrix} v_{1} - w_{1} \\ v_{2} - w_{2} \\ \vdots \end{bmatrix}$
      \item Scalar multiplication: $\alpha v = \begin{bmatrix} \alpha v_{1} \\ \alpha v_{2} \\ \vdots \end{bmatrix}$
      \item Norm: "intuitively" represents the length of a vector, is a scalar value \\
      p-norm - $\left\Vert v \right\Vert_{p} = (\sum_{i=1}^{n} \abs{v_{i}}^{p})^{\frac{1}{p}}$ \\
      1-norm - $\left\Vert v \right\Vert_{1} = (\sum_{i=1}^{n} \abs{v_{i}})$ \\
      2-norm - $\left\Vert v \right\Vert_{2} = (\sum_{i=1}^{n} \abs{v_{i}}^{2})^{\frac{1}{2}}$
      \item Unit vector: a vector with Euclidean norm of 1 ($\left\Vert v \right\Vert = 1$)
      \item \indent unit vectors are used to describe directions in coordinate frames and transforms
  \end{itemize}

  \textbf{Basis Vectors} \\

  A set of vectors is said to be linearly independent if no vector is a linear combination of another other vectors in the set. \\

  e.g. $\{ \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} \}$ is linearly independent and commonly called the \textit{standard basis} for $\mathbb{R}^{3}$

  $\{ \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix} \}$ is not linearly indepdent because the last vector a linear combination of the first two, $2v_{1} + v{2} = v_{3}$

  A set of vectors $\mathcal{B} = \{b_{1}, b_{2}, ...\}$ spans a vector space if any vector in the space can be written as a linear combination of other vectors in the space. Formally, for any $v \in \mathbb{R}^{n}, v = \alpha_{1} b_{1} + \alpha_{2} b_{2} + \ldots$

  A basis of vector space $\mathbb{R}^{n}$ is a set of \underline{linearly independent} vectors that \underline{span} the entire space.

  Note: Basis vectors are not unique. (show example) \\


  \textbf{Vector dot product (inner product)}

  \begin{itemize}
  \item $v \cdot w = \langle v, w \rangle = \sum_{i=1}^{n} v_{i}w_{i}$

  \item The angle between two vectors is: $\theta = arccos(\frac{v \cdot w}{\norm{v}\norm{w}})$

  \item $v$ and $w$ are \underline{orthogonal} if $v \cdot w = 0$. The angle between two orthgonal vectors is $\frac{\pi}{2}$

  \item Dot product and scalar: $\alpha (v \cdot w) = (\alpha v) \cdot w = v \cdot (\alpha w)$

  \item Distribution over addition: $v \cdot (w + p) = v \cdot w + v \cdot p$
  \end{itemize}

  \textbf{Vector projection}

  [TODO]

  \textbf{Vector cross product}

  The cross product of two vectors results in a third vector that is orthogonal to both vectors. Apply right hand rule to determine the direction of the resulting vector.

  \begin{align*}
  c &= a \times b \\
  c &= \begin{bmatrix} a_{y}b_{z} - a_{z}b_{y} \\ a_{z}b_{x} - a_{x}b_{z} \\ a_{x}b_{y} - a_{y}b_{x}\end{bmatrix}
  \end{align*}

  \section{Matrices}

  A matrix is a rectangular array of values, two-dimensional vector. A vector is a matrix with 1 column. Like vectors, we can add matrices with the same dimensions together and multiply a matrix by a scalar value.

  \textbf{Matrix operations} \\
  For matrices A and B with dimensions $m \times n_{a}$ and $n_{b} \times k$ respectively.
  \begin{itemize}
      \item Multiplication: $(AB)_{ij} = \sum_{k=1}^{n_{a}} a_{ik} b_{kj}$
      \begin{itemize}
          \item Multiplication is not commutative! You can only multiple two matrices if the number of columns of matrix A is equal to the number of rows in matrix B (i.e. $n_{a} = n_{b}$)
      \end{itemize}
      \item Transpose: the transpose of a matrix is done by flipping a matrix over its diagonal such that $[A^{T}]_{ij} = A_{ji}$
      \begin{itemize}
          \item e.g. [todo]
          \item $(AB)^{T} = B^{T}A^{T}$
          \item $(A^{T})^{T} = A$
      \end{itemize}
      \item Identity: the identity matrix ($I_{n}$) is an $n \times n$ matrix with 1's along its diagonal and 0's everywhere else
      \begin{itemize}
          \item e.g. $\begin{bmatrix} 1&0&0 \\ 0&1&0 \\ 0&0&1 \end{bmatrix}$
      \end{itemize}
      \item Inversion: $A^{-1}$ is the inverse of $A$ if $A^{-1}A = AA^{-1} = I$
      \begin{itemize}
          \item To have an inverse, $A$ must be a square matrix and invertible.
          \item A square matrix is \underline{singular} if it is not invertible.
          \item A square matrix is invertible, if either it has rank \textit{n} or if its determinant is 0. There are many other ways to check for invertability.
          \item Matrix inversion is commonly used in linear algebra to solve a system of linear equations. $Ax = b \rightarrow x = A^{-1}b$
      \end{itemize}
  \end{itemize}

  \textbf{Pseudo-inverse}
  \begin{itemize}
      \item The Moore-Penrose Pseudo-inverse is defined as $A^{+} = (A^{T}A)^{-1}A^{T}$ (left pseudo-inverse)
      \item The Moore-Pensore Pseudo-inverse works even when $A$ is not a square matrix. If $A$ is square and invertible, then $A^{+} = A^{-1}$.
      \item [TODO] write about underdetermined systems
  \end{itemize}
\end{document}